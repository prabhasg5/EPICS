{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prabhasg5/EPICS/blob/main/fashion_qgan_online.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e01c759b",
      "metadata": {
        "id": "e01c759b"
      },
      "source": [
        "# QGAN for Fashion Design (FashionGen-compatible)\n",
        "\n",
        "This notebook implements a Quantum GAN (QGAN) where the Generator is a quantum variational circuit (PennyLane) wrapped into a PyTorch module and the Discriminator is a classical convolutional neural network (PyTorch).\n",
        "\n",
        "Goal: train on the FashionGen dataset if provided (path above). If FashionGen isn't available locally, the notebook no longer falls back to Fashion-MNIST ‚Äî the notebook requires the FashionGen dataset to be available locally.\n",
        "\n",
        "What this notebook includes:\n",
        "- Setup and dependency checks\n",
        "- Dataset loader that accepts a FashionGen root path (images) and auto-detects flat or folder layouts\n",
        "- Quantum Generator implemented with PennyLane's TorchLayer\n",
        "- Classical Discriminator (PyTorch)\n",
        "- Training loop with checkpoints and sample visualization\n",
        "\n",
        "Notes & assumptions:\n",
        "- FashionGen is large. Point `FASHIONGEN_PATH` (or place files in `./fashion-dataset/images`) to your local FashionGen images arranged as images or subfolders.\n",
        "- The quantum circuit maps a small latent vector into features; a classical head upsamples to full image size.\n",
        "- For a research review, run the full training on GPU (if available) and with the full FashionGen dataset. The notebook provides the training scaffolding and checkpointing.\n",
        "\n",
        "Please run the cells in order from top to bottom."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -q kagglehub torchvision\n"
      ],
      "metadata": {
        "id": "dC3c36Veqfme"
      },
      "id": "dC3c36Veqfme",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "75bc84ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75bc84ba",
        "outputId": "12211af6-a6c7-44e2-835b-67a242043457"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing pennylane...\n",
            "torch already installed\n",
            "torchvision already installed\n",
            "matplotlib already installed\n",
            "PIL already installed\n",
            "============================================================\n",
            "Device: cpu\n",
            "‚ö†Ô∏è WARNING: GPU not available! Using CPU.\n",
            "============================================================\n",
            "‚úÖ Setup complete\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pennylane/__init__.py:209: RuntimeWarning: PennyLane is not yet compatible with JAX versions > 0.6.2. You have version 0.7.2 installed. Please downgrade JAX to 0.6.2 to avoid runtime errors using python -m pip install jax~=0.6.0 jaxlib~=0.6.0\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import subprocess\n",
        "import importlib\n",
        "\n",
        "def ensure_package(pkg_name, pip_name=None):\n",
        "    pip_name = pip_name or pkg_name\n",
        "    try:\n",
        "        importlib.import_module(pkg_name)\n",
        "        print(f\"{pkg_name} already installed\")\n",
        "    except Exception:\n",
        "        print(f\"Installing {pip_name}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name])\n",
        "\n",
        "# Install packages\n",
        "ensure_package('pennylane')\n",
        "ensure_package('torch')\n",
        "ensure_package('torchvision')\n",
        "ensure_package('matplotlib')\n",
        "ensure_package('PIL', 'pillow')\n",
        "\n",
        "# Imports\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as pnp\n",
        "from pennylane.qnn import TorchLayer\n",
        "\n",
        "# ===== GPU SETUP =====\n",
        "# Check CUDA availability and GPU info\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('=' * 60)\n",
        "print(f'Device: {device}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
        "    print(f'CUDA Version: {torch.version.cuda}')\n",
        "    print(f'Available GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')\n",
        "    # Enable TF32 for better performance on Ampere GPUs (T4 doesn't have it, but safe to enable)\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "    # cuDNN optimization\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "else:\n",
        "    print('‚ö†Ô∏è WARNING: GPU not available! Using CPU.')\n",
        "print('=' * 60)\n",
        "\n",
        "# Reproducibility\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Utility: show image grid\n",
        "def show_image_tensor(img_tensor, nrow=8, title=None, denorm=True):\n",
        "    if denorm:\n",
        "        img_tensor = (img_tensor + 1.0) / 2.0\n",
        "    grid = make_grid(img_tensor.cpu(), nrow=nrow, normalize=False)\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.axis('off')\n",
        "    if title: plt.title(title)\n",
        "    plt.imshow(grid.permute(1,2,0).clamp(0,1))\n",
        "    plt.show()\n",
        "\n",
        "print('‚úÖ Setup complete')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "import kagglehub\n",
        "\n",
        "# Download dataset\n",
        "path = kagglehub.dataset_download(\"paramaggarwal/fashion-product-images-dataset\")\n",
        "print(\"‚úÖ Kaggle dataset downloaded to:\", path)\n",
        "\n",
        "data_root = os.path.join(path, \"images\")\n",
        "if not os.path.exists(data_root):\n",
        "    data_root = path\n",
        "print(\"üìÅ Using data root:\", data_root)\n",
        "\n",
        "# ===== GPU-OPTIMIZED SETTINGS =====\n",
        "image_size = 28  # Keep at 28 for quantum circuit compatibility; increase to 64/128 for better quality\n",
        "batch_size = 128  # ‚¨ÜÔ∏è Increased from 64 - T4 can handle larger batches\n",
        "num_workers = 2   # ‚¨ÜÔ∏è Increased from 0 - better CPU->GPU pipeline\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Helper: FlatImageDataset\n",
        "class FlatImageDataset(Dataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.root = root\n",
        "        self.paths = sorted(\n",
        "            glob(os.path.join(root, \"*.jpg\")) +\n",
        "            glob(os.path.join(root, \"*.jpeg\")) +\n",
        "            glob(os.path.join(root, \"*.png\"))\n",
        "        )\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, 0\n",
        "\n",
        "# Detect structure\n",
        "has_subdirs = any(os.path.isdir(os.path.join(data_root, p)) for p in os.listdir(data_root))\n",
        "if has_subdirs:\n",
        "    print(f\"üì¶ Loading dataset using ImageFolder from {data_root}\")\n",
        "    dataset = datasets.ImageFolder(root=data_root, transform=transform)\n",
        "else:\n",
        "    print(f\"üì∑ Loading dataset from flat folder: {data_root}\")\n",
        "    dataset = FlatImageDataset(data_root, transform=transform)\n",
        "\n",
        "if len(dataset) == 0:\n",
        "    raise RuntimeError(f\"No images found in {data_root}\")\n",
        "\n",
        "# ===== GPU-OPTIMIZED DATALOADER =====\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True,          # ‚úÖ Speeds up CPU->GPU transfer\n",
        "    persistent_workers=(num_workers > 0),\n",
        "    prefetch_factor=2 if num_workers > 0 else None  # ‚úÖ Prefetch batches\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Dataset size: {len(dataset)}\")\n",
        "\n",
        "# Detect channels\n",
        "batch_example = next(iter(dataloader))\n",
        "example_imgs = batch_example[0]\n",
        "detected_channels = example_imgs.shape[1]\n",
        "detected_size = example_imgs.shape[2]\n",
        "print(f\"üîç Detected: channels={detected_channels}, image_size={detected_size}\")\n",
        "\n",
        "num_channels = detected_channels\n",
        "image_size = detected_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "hrHIsCjwrL4l",
        "outputId": "afdc0c7c-9181-48ab-91c8-94107dbaeedb"
      },
      "id": "hrHIsCjwrL4l",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/paramaggarwal/fashion-product-images-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 16.7G/23.1G [07:51<02:58, 38.1MB/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3778563678.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Download dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkagglehub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"paramaggarwal/fashion-product-images-dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úÖ Kaggle dataset downloaded to:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/datasets.py\u001b[0m in \u001b[0;36mdataset_download\u001b[0;34m(handle, path, force_download)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_dataset_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Downloading Dataset: {h.to_url()} ...\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mEXTRA_CONSOLE_BLOCK\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_resolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/registry.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimpl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/resolver.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, handle, path, force_download)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mSome\u001b[0m \u001b[0mcases\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mversion\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mmight\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCompetition\u001b[0m \u001b[0mdatasource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAPI\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbased\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \"\"\"\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Note handles are immutable, so _resolve() could not have altered our reference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/http_resolver.py\u001b[0m in \u001b[0;36m_resolve\u001b[0;34m(self, h, path, force_download)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;31m# First, we download the archive.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mapi_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchive_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0m_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/clients.py\u001b[0m in \u001b[0;36mdownload_file\u001b[0;34m(self, path, out_file, resource_handle, cached_path, extract_auto_compressed_file)\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Downloading from {url}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 \u001b[0m_download_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhash_object\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/clients.py\u001b[0m in \u001b[0;36m_download_file\u001b[0;34m(response, out_file, size_read, total_size, hash_object)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit_divisor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHUNK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mhash_object\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mChunkedEncodingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoded_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    978\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoded_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m         \u001b[0mflush_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_error_catcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    885\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m             \u001b[0;31m# StringIO doesn't like amt=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m     def _raw_read(\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;31m# clip the read to the \"end of response\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m                 \u001b[0mamt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m                 \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1249\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1101\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3af8d40f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3af8d40f",
        "outputId": "64304c33-ae1a-411b-80eb-31755be41cc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming download from 17975738368 bytes (6795477372 bytes left)...\n",
            "Resuming download from https://www.kaggle.com/api/v1/datasets/download/paramaggarwal/fashion-product-images-dataset?dataset_version_number=1 (17975738368/24771215740) bytes left.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23.1G/23.1G [02:48<00:00, 40.3MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Kaggle dataset downloaded to: /root/.cache/kagglehub/datasets/paramaggarwal/fashion-product-images-dataset/versions/1\n",
            "üìÅ Using data root: /root/.cache/kagglehub/datasets/paramaggarwal/fashion-product-images-dataset/versions/1\n",
            "üì¶ Loading dataset using ImageFolder from /root/.cache/kagglehub/datasets/paramaggarwal/fashion-product-images-dataset/versions/1\n",
            "‚úÖ Dataset size: 88882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Detected: channels=3, image_size=28\n"
          ]
        }
      ],
      "source": [
        "from glob import glob\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "import kagglehub\n",
        "\n",
        "# Download dataset\n",
        "path = kagglehub.dataset_download(\"paramaggarwal/fashion-product-images-dataset\")\n",
        "print(\"‚úÖ Kaggle dataset downloaded to:\", path)\n",
        "\n",
        "data_root = os.path.join(path, \"images\")\n",
        "if not os.path.exists(data_root):\n",
        "    data_root = path\n",
        "print(\"üìÅ Using data root:\", data_root)\n",
        "\n",
        "# ===== GPU-OPTIMIZED SETTINGS =====\n",
        "image_size = 28  # Keep at 28 for quantum circuit compatibility; increase to 64/128 for better quality\n",
        "batch_size = 128  # ‚¨ÜÔ∏è Increased from 64 - T4 can handle larger batches\n",
        "num_workers = 0   # Set to 0 for Colab stability (multiprocessing issues)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Helper: FlatImageDataset\n",
        "class FlatImageDataset(Dataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.root = root\n",
        "        self.paths = sorted(\n",
        "            glob(os.path.join(root, \"*.jpg\")) +\n",
        "            glob(os.path.join(root, \"*.jpeg\")) +\n",
        "            glob(os.path.join(root, \"*.png\"))\n",
        "        )\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, 0\n",
        "\n",
        "# Detect structure\n",
        "has_subdirs = any(os.path.isdir(os.path.join(data_root, p)) for p in os.listdir(data_root))\n",
        "if has_subdirs:\n",
        "    print(f\"üì¶ Loading dataset using ImageFolder from {data_root}\")\n",
        "    dataset = datasets.ImageFolder(root=data_root, transform=transform)\n",
        "else:\n",
        "    print(f\"üì∑ Loading dataset from flat folder: {data_root}\")\n",
        "    dataset = FlatImageDataset(data_root, transform=transform)\n",
        "\n",
        "if len(dataset) == 0:\n",
        "    raise RuntimeError(f\"No images found in {data_root}\")\n",
        "\n",
        "# ===== GPU-OPTIMIZED DATALOADER =====\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True,          # ‚úÖ Speeds up CPU->GPU transfer\n",
        "    persistent_workers=False  # Must be False when num_workers=0\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Dataset size: {len(dataset)}\")\n",
        "\n",
        "# Detect channels\n",
        "batch_example = next(iter(dataloader))\n",
        "example_imgs = batch_example[0]\n",
        "detected_channels = example_imgs.shape[1]\n",
        "detected_size = example_imgs.shape[2]\n",
        "print(f\"üîç Detected: channels={detected_channels}, image_size={detected_size}\")\n",
        "\n",
        "num_channels = detected_channels\n",
        "image_size = detected_size\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# After mounting\n",
        "checkpoint_path = \"/content/drive/MyDrive/qgan_training/checkpoint_latest.pth\"\n",
        "os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjkY7rE588Pn",
        "outputId": "25579001-2984-4078-f02e-46517f358770"
      },
      "id": "EjkY7rE588Pn",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f8948438",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8948438",
        "outputId": "6931db52-bb5e-469a-ba37-fbe4ca57b7ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QuantumGenerator(\n",
            "  (pre): Linear(in_features=8, out_features=4, bias=True)\n",
            "  (qlayer): <Quantum Torch Layer: func=qcircuit>\n",
            "  (post): Sequential(\n",
            "    (0): Linear(in_features=4, out_features=256, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Linear(in_features=256, out_features=2352, bias=True)\n",
            "    (3): Tanh()\n",
            "  )\n",
            ")\n",
            "‚úÖ Generator on: cpu\n",
            "‚úÖ Fake image shape: torch.Size([4, 3, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "latent_dim = 8\n",
        "n_qubits = 4\n",
        "n_layers = 3\n",
        "image_dim = num_channels * image_size * image_size\n",
        "\n",
        "# ===== QUANTUM DEVICE =====\n",
        "# NOTE: PennyLane's quantum circuits run on CPU, but we optimize data flow\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "@qml.qnode(dev, interface=\"torch\", diff_method=\"backprop\")\n",
        "def qcircuit(inputs, weights):\n",
        "    qml.templates.AngleEmbedding(inputs, wires=range(n_qubits))\n",
        "    qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
        "\n",
        "weight_shapes = {\"weights\": (n_layers, n_qubits, 3)}\n",
        "qlayer = qml.qnn.TorchLayer(qcircuit, weight_shapes)\n",
        "\n",
        "class QuantumGenerator(nn.Module):\n",
        "    def __init__(self, latent_dim=latent_dim, n_qubits=n_qubits,\n",
        "                 image_dim=image_dim, out_channels=num_channels):\n",
        "        super().__init__()\n",
        "        self.pre = nn.Linear(latent_dim, n_qubits)\n",
        "        self.qlayer = qlayer\n",
        "        post_hidden = 256\n",
        "        self.post = nn.Sequential(\n",
        "            nn.Linear(n_qubits, post_hidden),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(post_hidden, image_dim),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "    def forward(self, z):\n",
        "        x = self.pre(z)\n",
        "        x = self.qlayer(x)  # Quantum layer (CPU bottleneck)\n",
        "        out = self.post(x)\n",
        "        out = out.view(-1, self.out_channels, image_size, image_size)\n",
        "        return out\n",
        "\n",
        "# ===== INITIALIZE ON GPU =====\n",
        "gen = QuantumGenerator().to(device)\n",
        "print(gen)\n",
        "print(f\"‚úÖ Generator on: {next(gen.parameters()).device}\")\n",
        "\n",
        "# Smoke test\n",
        "with torch.no_grad():\n",
        "    sample_z = torch.randn(4, latent_dim, device=device)\n",
        "    fake = gen(sample_z)\n",
        "    print(f'‚úÖ Fake image shape: {fake.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, img_channels=num_channels):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(img_channels, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512 * (image_size // 16) * (image_size // 16), 1),\n",
        "            # ‚úÖ REMOVED Sigmoid - use BCEWithLogitsLoss instead\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x).view(-1)\n",
        "\n",
        "# ===== INITIALIZE ON GPU =====\n",
        "disc = Discriminator().to(device)\n",
        "print(f\"‚úÖ Discriminator on: {next(disc.parameters()).device}\")\n",
        "\n",
        "# ===== GPU-OPTIMIZED OPTIMIZERS =====\n",
        "# Increased learning rate slightly for faster convergence\n",
        "optimG = torch.optim.Adam(gen.parameters(), lr=3e-4, betas=(0.5, 0.999))\n",
        "optimD = torch.optim.Adam(disc.parameters(), lr=3e-4, betas=(0.5, 0.999))\n",
        "\n",
        "# ‚úÖ USE BCEWithLogitsLoss (combines Sigmoid + BCE, safe for mixed precision)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "print(\"‚úÖ Models and optimizers ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fF2Y-o11Dpso",
        "outputId": "af0cf1e8-7fa5-4f7e-b21d-1530e6ca2008"
      },
      "id": "fF2Y-o11Dpso",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Discriminator on: cpu\n",
            "‚úÖ Models and optimizers ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Delete old checkpoint to start training from scratch\n",
        "# import os\n",
        "# if os.path.exists(checkpoint_path):\n",
        "#     os.remove(checkpoint_path)\n",
        "#     print(\"‚úÖ Old checkpoint deleted\")"
      ],
      "metadata": {
        "id": "-nZQ1uI3M5XN"
      },
      "id": "-nZQ1uI3M5XN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"/content/checkpoint_latest.pth\"\n",
        "save_every_batches = 500\n",
        "atomic_tmp = checkpoint_path + \".tmp\"\n",
        "\n",
        "def save_checkpoint(path, epoch, batch_idx, gen, disc, optimG, optimD):\n",
        "    ckpt = {\n",
        "        \"epoch\": epoch,\n",
        "        \"batch_idx\": batch_idx,\n",
        "        \"gen_state\": gen.state_dict(),\n",
        "        \"disc_state\": disc.state_dict(),\n",
        "        \"optimG_state\": optimG.state_dict(),\n",
        "        \"optimD_state\": optimD.state_dict(),\n",
        "        \"torch_rng\": torch.get_rng_state(),\n",
        "        \"cuda_rng_all\": torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None,\n",
        "        \"np_rng\": np.random.get_state(),\n",
        "        \"py_random\": random.getstate(),\n",
        "    }\n",
        "    torch.save(ckpt, atomic_tmp)\n",
        "    os.replace(atomic_tmp, path)\n",
        "    print(f\"üíæ Saved checkpoint -> {path} (epoch={epoch}, batch={batch_idx})\")\n",
        "\n",
        "def load_checkpoint(path, gen, disc, optimG, optimD, device):\n",
        "    if not os.path.exists(path):\n",
        "        return 1, 0\n",
        "    print(\"üì• Loading checkpoint:\", path)\n",
        "\n",
        "    # Fix for PyTorch 2.6+ weights_only default change\n",
        "    # Safe to set False since we trust our own checkpoints\n",
        "    ckpt = torch.load(path, map_location=device, weights_only=False)\n",
        "\n",
        "    gen.load_state_dict(ckpt[\"gen_state\"])\n",
        "    disc.load_state_dict(ckpt[\"disc_state\"])\n",
        "    optimG.load_state_dict(ckpt[\"optimG_state\"])\n",
        "    optimD.load_state_dict(ckpt[\"optimD_state\"])\n",
        "\n",
        "    # Fix for RNG state type compatibility\n",
        "    try:\n",
        "        torch_rng = ckpt[\"torch_rng\"]\n",
        "        if isinstance(torch_rng, np.ndarray):\n",
        "            torch_rng = torch.from_numpy(torch_rng)\n",
        "        torch.set_rng_state(torch_rng)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not restore torch RNG state: {e}\")\n",
        "\n",
        "    try:\n",
        "        if torch.cuda.is_available() and ckpt.get(\"cuda_rng_all\") is not None:\n",
        "            cuda_rng = ckpt[\"cuda_rng_all\"]\n",
        "            if isinstance(cuda_rng, list):\n",
        "                cuda_rng = [torch.from_numpy(s) if isinstance(s, np.ndarray) else s for s in cuda_rng]\n",
        "            torch.cuda.set_rng_state_all(cuda_rng)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not restore CUDA RNG state: {e}\")\n",
        "\n",
        "    try:\n",
        "        np.random.set_state(ckpt[\"np_rng\"])\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not restore numpy RNG state: {e}\")\n",
        "\n",
        "    try:\n",
        "        random.setstate(ckpt[\"py_random\"])\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not restore Python RNG state: {e}\")\n",
        "\n",
        "    start_epoch = int(ckpt.get(\"epoch\", 1))\n",
        "    start_batch = int(ckpt.get(\"batch_idx\", 0))\n",
        "    print(f\"‚úÖ Resuming from epoch={start_epoch}, batch={start_batch}\")\n",
        "    return start_epoch, start_batch\n",
        "\n",
        "start_epoch, start_batch = load_checkpoint(checkpoint_path, gen, disc, optimG, optimD, device)\n",
        "\n"
      ],
      "metadata": {
        "id": "OCbG1tKR8AKG"
      },
      "id": "OCbG1tKR8AKG",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edec7ae5",
      "metadata": {
        "id": "edec7ae5"
      },
      "outputs": [],
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# ===== MIXED PRECISION TRAINING =====\n",
        "# Use updated API for PyTorch 2.0+\n",
        "use_amp = torch.cuda.is_available()\n",
        "try:\n",
        "    # New API (PyTorch 2.0+)\n",
        "    scaler = torch.amp.GradScaler('cuda', enabled=use_amp)\n",
        "except AttributeError:\n",
        "    # Fallback for older PyTorch versions\n",
        "    scaler = GradScaler(enabled=use_amp)\n",
        "\n",
        "# Use new autocast API\n",
        "try:\n",
        "    from torch.amp import autocast\n",
        "    autocast_context = lambda: autocast('cuda', enabled=use_amp)\n",
        "except ImportError:\n",
        "    from torch.cuda.amp import autocast\n",
        "    autocast_context = lambda: autocast(enabled=use_amp)\n",
        "\n",
        "print(f\"üöÄ Mixed Precision Training: {'ENABLED' if use_amp else 'DISABLED'}\")\n",
        "\n",
        "# Training hyperparams\n",
        "num_epochs = 10\n",
        "print_every = 50\n",
        "\n",
        "# Track timing\n",
        "epoch_start_time = time.time()\n",
        "batch_count = 0\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs + 1):\n",
        "    for batch_idx, (real_imgs, _) in enumerate(dataloader):\n",
        "        # Skip batches if resuming\n",
        "        if epoch == start_epoch and batch_idx < start_batch:\n",
        "            continue\n",
        "\n",
        "        batch_count += 1\n",
        "        current_batch_size = real_imgs.size(0)\n",
        "\n",
        "        # ===== MOVE DATA TO GPU =====\n",
        "        real_imgs = real_imgs.to(device, non_blocking=True)  # ‚úÖ non_blocking for speed\n",
        "\n",
        "        # Labels\n",
        "        real_labels = torch.ones(current_batch_size, device=device)\n",
        "        fake_labels = torch.zeros(current_batch_size, device=device)\n",
        "\n",
        "        # ========== Train Discriminator ==========\n",
        "        optimD.zero_grad(set_to_none=True)  # ‚úÖ Faster than zero_grad()\n",
        "\n",
        "        with autocast_context():  # ‚úÖ Mixed precision\n",
        "            # Real images\n",
        "            real_out = disc(real_imgs)\n",
        "            loss_real = criterion(real_out, real_labels)\n",
        "\n",
        "            # Fake images\n",
        "            z = torch.randn(current_batch_size, latent_dim, device=device)\n",
        "            fake_imgs = gen(z).detach()\n",
        "            fake_out = disc(fake_imgs)\n",
        "            loss_fake = criterion(fake_out, fake_labels)\n",
        "\n",
        "            loss_D = loss_real + loss_fake\n",
        "\n",
        "        scaler.scale(loss_D).backward()\n",
        "        scaler.step(optimD)\n",
        "        scaler.update()\n",
        "\n",
        "        # ========== Train Generator ==========\n",
        "        optimG.zero_grad(set_to_none=True)\n",
        "\n",
        "        with autocast_context():\n",
        "            z = torch.randn(current_batch_size, latent_dim, device=device)\n",
        "            fake_imgs = gen(z)\n",
        "            fake_out = disc(fake_imgs)\n",
        "            loss_G = criterion(fake_out, real_labels)\n",
        "\n",
        "        scaler.scale(loss_G).backward()\n",
        "        scaler.step(optimG)\n",
        "        scaler.update()\n",
        "\n",
        "        # ========== Logging ==========\n",
        "        if batch_count % print_every == 0:\n",
        "            elapsed = time.time() - epoch_start_time\n",
        "            batches_per_sec = batch_count / elapsed\n",
        "            print(f\"[Epoch {epoch}/{num_epochs}] [Batch {batch_idx}/{len(dataloader)}] \"\n",
        "                  f\"D_loss: {loss_D.item():.4f} | G_loss: {loss_G.item():.4f} \"\n",
        "                  f\"| Speed: {batches_per_sec:.2f} batches/s\")\n",
        "\n",
        "        # ========== Checkpointing ==========\n",
        "        if batch_count % save_every_batches == 0:\n",
        "            save_checkpoint(checkpoint_path, epoch, batch_idx, gen, disc, optimG, optimD)\n",
        "\n",
        "    # End of epoch\n",
        "    save_checkpoint(checkpoint_path, epoch, len(dataloader), gen, disc, optimG, optimD)\n",
        "    print(f\"‚úÖ Epoch {epoch} complete\")\n",
        "\n",
        "    # Show generated samples\n",
        "    with torch.no_grad():\n",
        "        sample_z = torch.randn(16, latent_dim, device=device)\n",
        "        samples = gen(sample_z)\n",
        "        show_image_tensor(samples, nrow=4, title=f'Generated Samples - Epoch {epoch}')\n",
        "\n",
        "print(\"üéâ Training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bc97862",
      "metadata": {
        "id": "3bc97862"
      },
      "outputs": [],
      "source": [
        "def load_checkpoint_for_inference(path, gen, disc, optG=None, optD=None, map_location=None):\n",
        "    \"\"\"Load checkpoint for inference or resuming training\"\"\"\n",
        "    ckpt = torch.load(path, map_location=map_location)\n",
        "    gen.load_state_dict(ckpt['gen_state'])\n",
        "    disc.load_state_dict(ckpt['disc_state'])\n",
        "    if optG and 'optimG_state' in ckpt:\n",
        "        optG.load_state_dict(ckpt['optimG_state'])\n",
        "    if optD and 'optimD_state' in ckpt:\n",
        "        optD.load_state_dict(ckpt['optimD_state'])\n",
        "    print(f'‚úÖ Loaded checkpoint: {path}')\n",
        "    return ckpt.get('epoch', None)\n",
        "\n",
        "# Create fixed noise for consistent visualization\n",
        "fixed_noise = torch.randn(64, latent_dim, device=device)\n",
        "\n",
        "# Try to load the latest checkpoint\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(\"üì• Loading checkpoint for visualization...\")\n",
        "    load_checkpoint_for_inference(checkpoint_path, gen, disc, optimG, optimD, map_location=device)\n",
        "\n",
        "    # Generate samples\n",
        "    gen.eval()  # Set to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        samples = gen(fixed_noise)\n",
        "        show_image_tensor(samples, nrow=8, title='Generated Samples from Checkpoint')\n",
        "    gen.train()  # Back to training mode\n",
        "else:\n",
        "    print('‚ö†Ô∏è No checkpoint found at:', checkpoint_path)\n",
        "    print('üí° Generate some samples with the untrained model:')\n",
        "    with torch.no_grad():\n",
        "        samples = gen(fixed_noise[:16])\n",
        "        show_image_tensor(samples, nrow=4, title='Samples (Untrained Model)')\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('‚úÖ Notebook ready!')\n",
        "print('='*60)\n",
        "print('üìù Training tips:')\n",
        "print('  ‚Ä¢ Current batch size: 128 (optimal for T4 GPU)')\n",
        "print('  ‚Ä¢ Mixed precision: ENABLED for 2-3x speed boost')\n",
        "print('  ‚Ä¢ Checkpoints save every 500 batches')\n",
        "print('  ‚Ä¢ To train longer: increase num_epochs and rerun training cell')\n",
        "print('='*60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}